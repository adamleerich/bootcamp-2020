---
title: "Ratemaking Capstone"
author: "Robert Riesenberg"
date: "9/26/2020"
output: html_document
---

```{r include=FALSE}
library(MASS)
library(gridExtra)
library(kableExtra)
library(ggthemes)
library(lubridate)
library(raw)
library(rpart)
library(rpart.plot)
library(actuar)
library(stats)
library(knitr)
library(GGally)
library(DataExplorer)
library(ggfortify)
library(broom)
library(Boruta)
library(tidyverse)

DirectoryPath <- getwd()
pol_final <- read_csv(file.path(DirectoryPath ,'pol_final.csv'))
claims_final <- read_csv(file.path(DirectoryPath ,'claims_final.csv'))

```


## Overview

Creating a frequency-severity analysis for a pure premium analysis by limit.

## Exploratory Data Analysis

Briefly looking over the data beforehand.


```{r warning=FALSE ,message=FALSE}

## Checking to see if there are any missing values
claims_final %>%
  plot_missing()

```

There are no missing values in the policy or claim dataset so the data is extremely clean. 

Looking over the distribution of the variables.

```{r warning=FALSE ,message=FALSE}

claims_final %>%
  select(-c(inception ,expiration)) %>%
  plot_histogram()


claims_final %>%
  plot_bar()

```


After seeing how the variables are distributed, want to check the correlations b/w variables.

```{r}
claims_final %>%
  plot_correlation(type = 'continuous')

```


## Severity Distribution

Want to fit a distribution to the claims ultimate severity data. Will be testing Gamma, Lognormal & Weibull.

```{r warning=FALSE}
fitGamma <- fitdistr(claims_final$claim_ultimate ,dgamma ,list(shape = 1.126754, rate = .01) ,lower = .0025)
fitLognormal <- fitdistr(claims_final$claim_ultimate ,"lognormal")
fitWeibull <- fitdistr(claims_final$claim_ultimate ,"Weibull")

## Probabilities ranging from 0 to 1
sims <- nrow(claims_final)
probabilities = seq_len(sims)/(sims + 1)

weibullQ <- qweibull(probabilities, coef(fitWeibull)[1], coef(fitWeibull)[2])
lnQ <- qlnorm(probabilities, coef(fitLognormal)[1], coef(fitLognormal)[2])
gammaQ <- qgamma(probabilities, coef(fitGamma)[1], coef(fitGamma)[2])

sortedSeverity <- sort(claims_final$claim_ultimate)

tibble(
  x          = c(sort(weibullQ) ,sort(lnQ) ,sort(gammaQ))
  ,y         = rep(sortedSeverity ,3)
  ,ModelType = c(rep("Weibull" ,length(sortedSeverity)) ,rep("LogNormal" ,length(sortedSeverity)) ,rep("Gamma" ,length(sortedSeverity)))
) %>%
  ggplot(aes(x ,y ,color = ModelType)) +
  geom_point() +
  geom_abline(intercept = 0 ,slope = 1) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  geom_jitter() +
  theme_fivethirtyeight() +
  ggtitle("Severity Distributions (Q-Q plot)")

```

```{r include=FALSE}
## Kolmogorov-Smirnov (K-S test)
## Test of fit
# sampleCumul <- seq(1, length(claims_final$claim_ultimate)) / length(claims_final$claim_ultimate)
# stepSample  <- stepfun(sortedSeverity, c(0, sampleCumul), f = 0)
# yGamma <- pgamma(sortedSeverity, sampleShape, sampleRate)
# yWeibull <- pweibull(sortedSeverity, sampleShapeW, sampleScaleW)
# yLN <- plnorm(sortedSeverity, sampleLogMean, sampleLogSd)
# 
# # plot(stepSample, col = "black", main = "K-S Gamma")
# # lines(sortedSeverity, yGamma, col = "blue")
# # 
# # plot(stepSample, col = "black", main = "K-S Weibull")
# # lines(sortedSeverity, yWeibull, col = "blue")
# # 
# # plot(stepSample, col = "black", main = "K-S Lognormal")
# # lines(sortedSeverity, yLN, col = "blue")
# 
# testGamma <- ks.test(claims_final$claim_ultimate, "pgamma", sampleShape, sampleRate)
# testLN <- ks.test(claims_final$claim_ultimate, "plnorm", sampleLogMean, sampleLogSd)
# testWeibull <- ks.test(claims_final$claim_ultimate, "pweibull", sampleShapeW, sampleScaleW)
# 
# tibble(
#   ModelType = c("Gamma" ,"LogNormal" ,"Weibull")
#   ,pValue   = c(testGamma$p.value ,testLN$p.value ,testWeibull$p.value)
# ) %>%
#   kable() %>%
#   kableExtra::kable_styling()
# testGamma
# testLN
# testWeibull
```

After determining that the Lognormal fit was best, compared the fitted Lognormal density to the actual density.

```{r warning=FALSE}

sampleLogMean <- fitLognormal$estimate[1]
sampleLogSd <- fitLognormal$estimate[2]

tibble(
  x = sortedSeverity
) %>%
  ggplot(aes(x ,color = "Actual")) +
  geom_density() +
  coord_cartesian(xlim = c(0 ,500000)) +
  stat_function(fun = dlnorm ,args = list(meanlog = sampleLogMean ,sdlog = sampleLogSd) ,aes(color = "Fitted")) +
  scale_color_manual(values = c("black" ,"blue")) +
  scale_x_continuous(labels = scales::comma) +
  ggtitle("Density Plot --Actual vs LN Fit") +
  theme_fivethirtyeight()
  
```

Severity distribution is looking pretty good. 

## Frequency GLM

Want to investigate to see if there are any add'l variables which should be added to the frequency GLM.

```{r}
## Adding add'l variables
pol_final <- pol_final %>%
  mutate(Frequency         = claim_count / revenue * 1000000
         ,RevenueGreater4m = if_else(revenue >= 4000000 ,1 ,0)
         ,EffectiveDate    = as.Date(str_c(str_sub(inception ,1 ,4) ,str_sub(inception ,-2 ,-1) ,"1" ,sep = "-"))
         ,ExpirationDate   = as.Date(str_c(str_sub(expiration ,1 ,4) ,str_sub(expiration ,-2 ,-1) ,"1" ,sep = "-"))
         ,EffectiveYear    = year(EffectiveDate)
         ,EffectiveMonth   = month(EffectiveDate)
         ,LengthInDays     = interval(EffectiveDate ,ExpirationDate) %/% days(1)
         )

pol_final_predictors_only <- pol_final %>%
  select(-c(policy_number ,inception ,expiration ,total_ultimate ,state ,average_severity ,Frequency)) %>%
  select(-c(contains("Effective") ,ExpirationDate ,LengthInDays))

tree_claim_count <- rpart(claim_count ~ . ,data = na.omit(pol_final_predictors_only) ,control = rpart.control(cp = .03))

rpart.plot(tree_claim_count)

```

Revenue being greater than $4M seems like it may be useful. Going to build models both with & without it in as an indicator to see if adding it is a good idea. Also will be checking frequency distributions & to see if adding an offset will increase performance.

```{r}
## Fitting various Poisson models
fit_poisson_no_rev_offset <- glm(formula = claim_count ~ . ,data = select(pol_final_predictors_only ,-RevenueGreater4m) ,family = "poisson" ,offset = log(revenue))
fit_poisson_rev_no_offset <- glm(formula = claim_count ~ . ,data = pol_final_predictors_only ,family = "poisson")
fit_poisson_rev_offset <- glm(formula = claim_count ~ . ,data = pol_final_predictors_only ,family = "poisson" ,offset = log(revenue))

## Fitting a negative binomial model
#### Offset not working with glm.nb function
#### Have to manually create the formula by adding in offset(log(revenue)) to formula
glm_nb_formula_with_offset <- pol_final_predictors_only %>% 
  select(-claim_count) %>%
  colnames() %>%
  str_c(collapse = " + ") %>%
  str_replace(pattern = "revenue" ,replacement = "offset(log(revenue))")
nb_glm_formula <- as.formula(str_c("claim_count ~ " ,glm_nb_formula_with_offset))
nb_glm_formula_less_vars <- as.formula(str_remove_all(str_c("claim_count ~ " ,glm_nb_formula_with_offset) ,pattern = "\\+ employee_count |\\+ year_started |\\+ use_written_contracts |\\+ years_in_business |\\+ five_year_claims"))

fit_neg_bin_rev_offset <- glm.nb(formula = nb_glm_formula ,data = pol_final_predictors_only)
fit_neg_bin_rev_offset_less_vars <- glm.nb(formula = nb_glm_formula_less_vars ,data = pol_final_predictors_only)

model_results <- bind_rows(glance(fit_poisson_no_rev_offset) ,glance(fit_poisson_rev_no_offset) ,glance(fit_poisson_rev_offset) ,glance(fit_neg_bin_rev_offset) ,glance(fit_neg_bin_rev_offset_less_vars)) 

model_results %>%
  bind_cols(ModelType                  = c("Poisson" ,"Poisson" ,"Poisson" ,"Negative Binomial" ,"Negative Binomial")
            ,Offset                    = c("log(revenue)" ,"None" ,"log(revenue)" ,"log(revenue)" ,"log(revenue)") 
            ,`Revenue >4M Flag`        = c("No" ,"Included" ,"Included" ,"Included" ,"Included")
            ,`Noise Variables Removed` = c("No" ,"No" ,"No" ,"No" ,"Yes")
            ) %>%
  select(ModelType ,`Revenue >4M Flag` ,Offset ,`Noise Variables Removed` ,AIC ,BIC ,logLik) %>%
  kable(digits = 0 ,format.args = list(big.mark = ',')) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  column_spec(5:7 ,color = "white" ,background = spec_color(model_results$AIC ,end = 0.7 ,direction = -1))


```

Having a Negative Binomial model with an offset for revenue, an indicator of whether revenue is greater than $4M, and removing less important variables seems to be the best model.

Need to look over the diagnostic plots.

```{r}

autoplot(fit_neg_bin_rev_offset_less_vars)

```

These diagonostics will be investigated further.

```{r message=FALSE ,warning=FALSE}
pol_final_chart <- pol_final %>%
  mutate(predicted_claim_count = predict(fit_neg_bin_rev_offset_less_vars ,type = "response"))

tibble(
  x     = c(pol_final_chart$claim_count ,pol_final_chart$predicted_claim_count)
  ,Type = c(rep("Actual" ,nrow(pol_final_chart)) ,rep("Predicted" ,nrow(pol_final_chart)))
) %>%
  ggplot(aes(x ,color = Type)) +
  geom_freqpoly() +
  scale_y_continuous(labels = scales::comma) +
  coord_cartesian(xlim = c(0 ,5)) +
  scale_color_manual(values = c("black" ,"blue")) +
  ggtitle("Actual vs Fitted --Claim Count") +
  theme_fivethirtyeight()

```

Predictions seem to imitate the data reasonably well.

## Pure Premiums

Calculating pure premium values at the policy level and examining pure premiums with limits by discipline.

```{r}
Lev1m <- levlnorm(1e6 ,meanlog = sampleLogMean ,sdlog = sampleLogSd)
Lev2m <- levlnorm(2e6 ,meanlog = sampleLogMean ,sdlog = sampleLogSd)
Lev5m <- levlnorm(5e6 ,meanlog = sampleLogMean ,sdlog = sampleLogSd)
UnlimLev <- mlnorm(1 ,meanlog = sampleLogMean ,sdlog = sampleLogSd)
 
pol_final_with_levs <- pol_final %>%
  mutate(predicted_claim_count  = predict(fit_neg_bin_rev_offset_less_vars ,type = "response")
         ,PurePrem1m            = predicted_claim_count * Lev1m
         ,PurePrem2m            = predicted_claim_count * Lev2m
         ,PurePrem5m            = predicted_claim_count * Lev5m
         ,UnlimPurePrem         = predicted_claim_count * UnlimLev
        )

pol_final_with_levs %>%
  select(policy_number ,discipline ,contains("PurePrem")) %>%
  gather(key = "Limit" ,value = "PurePremium" ,-c(policy_number ,discipline)) %>%
  mutate(Limit = str_remove_all(Limit ,pattern = "PurePrem")) %>%
  ggplot(aes(x = PurePremium ,y = discipline)) +
  geom_violin() +
  facet_wrap(~ Limit) +
  coord_cartesian(xlim = c(0 ,1e4)) +
  scale_x_continuous(labels = scales::comma) +
  ggtitle("Pure Premium Distributions by Limits") +
  theme_fivethirtyeight()
```  
  
Looking at the mean values.

```{r}

pol_final_with_levs %>%
  group_by(discipline) %>%
  summarise_at(vars(contains("PurePrem")) ,mean) %>%
  arrange(desc(UnlimPurePrem)) %>%
  kable(digits = 0 ,format.args = list(big.mark = ',')) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```
Can see that Structural Engineering has the highest pure premiums while Landscape Architecture has the lowest. Can also see that there isn't much difference by limit for the pure premiums.

```{r include = FALSE}
## Scratch work


# pol_final_with_levs %>%
#   group_by(discipline) %>%
#   summarise_at(vars(contains("PurePrem")) ,mean) %>%
#   arrange(desc(UnlimPurePrem))

# claims_final %>%
#   mutate(Capped1m  = pmin(claim_ultimate ,1e6)
#          ,Capped2m = pmin(claim_ultimate ,2e6)
#          ,Capped5m = pmin(claim_ultimate ,5e6)
#         ) %>%
#   summarise(Capped1mLoss  = sum(Capped1m)
#             ,Capped2mLoss = sum(Capped2m)
#             ,Capped5mLoss = sum(Capped5m))
# 
# 
# pol_final_with_levs %>%
#   summarise_at(vars(contains("PurePrem")) ,sum)

# log_like_gamma <- function(sample_in, shape_in, scale_in) {
#   dgamma(sample_in, shape_in, scale = scale_in, log = TRUE) %>% 
#     sum()
# }
# 
# ## Determining optimal value for scale
# tbl_log_like <- tibble(
#   shape = seq(0.05, 10, length.out = 500)
#   , scale = mean(claims_final$claim_ultimate) / shape
#   , log_like = map2_dbl(shape, scale, log_like_gamma, sample_in = claims_final$claim_ultimate)
# )
# 
# ## Shape is around 1 with best guess
# tbl_log_like %>% 
#   ggplot(aes(shape, log_like)) + 
#   geom_line()
# 
# tbl_log_like[which.max(tbl_log_like$log_like),]
# 
# ## Determining optimal value for shape
# tbl_log_like <- tibble(
#   scale = seq(0.01 * min(claims_final$claim_ultimate), max(claims_final$claim_ultimate) / 2, length.out = 500)
#   , shape = mean(claims_final$claim_ultimate) / scale
#   , log_like = map2_dbl(shape, scale, log_like_gamma, sample_in = claims_final$claim_ultimate)
# )
# 
# ## Shape is extremely flat on top
# tbl_log_like %>% 
#   ggplot(aes(scale, log_like)) + 
#   geom_line() +
#   coord_cartesian(xlim = c(0,50000))
# 
# ## Looking at both shape & scale
# tbl_log_like <- crossing(
#     scale = seq(0.5 * min(claims_final$claim_ultimate), max(claims_final$claim_ultimate), length.out = 100)
#   , shape = seq(0.05, 10, length.out = 100)
# ) %>% 
#   mutate(
#     log_like = map2_dbl(shape, scale, log_like_gamma, sample_in = claims_final$claim_ultimate)
#     , exp_sev = scale * shape
#   )
# 
# tbl_sample_mean <- tbl_log_like %>% 
#   filter(
#     abs(exp_sev - mean(claims_final$claim_ultimate)) < 10000
#   )
# 
# ## tbl_log_like %>%
# ##   ggplot(aes(scale, shape)) +
# ##   geom_raster(aes(fill = log_like), interpolate = TRUE) +
# ##   scale_fill_continuous(low = 'red', high = 'green') +
# ##   geom_line(data = tbl_sample_mean, color = 'black') +
# ##   theme_minimal()
# 
# ## Visualizing log_likihood for various shape & scale parameters
# ## Black line reproduces sample mean
# tbl_log_like %>% 
#   ggplot(aes(scale, shape)) + 
#   geom_raster(aes(fill = log_like), interpolate = TRUE) +
#   scale_fill_continuous(low = 'red', high = 'green') + 
#   geom_line(data = tbl_sample_mean, color = 'black') + 
#   theme_minimal()
# 
# 
# pol_final %>%
#   mutate(EffectiveDate   = as.Date(str_c(str_sub(inception ,1 ,4) ,str_sub(inception ,-2 ,-1) ,"1" ,sep = "-"))
#          ,ExpirationDate = as.Date(str_c(str_sub(expiration ,1 ,4) ,str_sub(expiration ,-2 ,-1) ,"1" ,sep = "-"))
#          ,LengthInDays   = interval(EffectiveDate ,ExpirationDate) %/% days(1)
#         ) %>%
#   select(EffectiveDate ,ExpirationDate ,LengthInDays ,everything()) %>%
#   summarise(Max    = max(LengthInDays)
#             ,Min   = min(LengthInDays)
#             ,Count = n()
#            )

```





