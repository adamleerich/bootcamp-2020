---
title: "Tree-based Methods: Part Two"
editor_options:
  chunk_output_type: console
title-slide-attributes:
  data-background-image: "../slide_input/background-forest-2.jpg"
  class: "center"
---


```{r include=FALSE}
source('common.R')
```



# Decision Trees Reviewed


## Required Packages
```{r eval=FALSE}
install.packages('tree')
library(tree)
library(tidyverse)
library(MASS)         # To get example data
library(raw)          # To get example data
```
```{r echo=FALSE}
library(tree)
library(tidyverse)
library(MASS)
library(raw)
```


## Other packages

* `rpart`
* `rpart.plot`
* `xgboost`





## Get Some Data
```{r}
data(Boston)
head(Boston)
```



## Now, you try!
* Go back to the `Boston` dataset in the `MASS` package
* Pick some variables to model house value against
* Plot something
* Create a tree and plot it



## A sample
* `crim` = per capita crime by town
* `ptratio` = parent teacher ratio
* `age` = proportion of owner-occupied units built prior to 1940
```{r}
bos <- Boston %>% 
  mutate(medv_pr = medv / rm) %>% 
  dplyr::select(age, dis, medv_pr) %>% 
  mutate(medv_prf = cut(medv_pr, breaks = 0:5 * 2))
```



## Maybe use ggplot2
```{r}
p <- ggplot(data = bos) +
  aes(x = age, y = dis, color = medv_pr) +
  geom_point(mapping = ) +
  scale_color_gradientn(
    colors = c('yellow', 'green', 'blue', 'red'))
```


## Plot
```{r out.height = "450px"}
print(p)
```



## What does the tree look like?
```{r}
tbos <- tree(
  formula = medv_pr ~ age + log(dis),
  data = bos
)
```



## Plot It
```{r out.height = "450px"}
plot(tbos, type = 'uniform')
text(tbos, pretty = 5, col = 'blue', cex = 0.8)
```



# Trees in Capstone Project

## Load up final policy data

Go ahead and clear your environment
using the RStudio button
or this command:

```{r}
rm(list = ls())
```

Then load the prepared data you got earlier today.

```{r eval=FALSE}
load('your_path/reshaped_data.Rdata')
ls()
```
```{r echo=FALSE}
load('c:/home/git/other/r_bootcamp/ratemaking-capstone/reshaped_data.Rdata')
ls()
```




## Understand Data

```{r}
str(pol_final)
```



## Select columns to include in tree

**How would you do this?  What columns should we include?**


## Let's get the data set ready

```{r}
pol_4tree <- pol_final %>% 
  mutate(has_claims = (claim_count > 0)) %>% 
  dplyr::select(
    has_claims, discipline, 
    employee_count, five_year_claims, 
    revenue, state_group, years_in_business,
    claim_count) %>% 
  mutate(
    has_claims = as.integer(has_claims),
    discipline = as.factor(discipline),
    state_group = as.factor(state_group))
```


## Throw it at `tree`

```{r}
tpol <- tree(
  formula = claim_count ~ revenue + discipline + state_group,
  data = pol_4tree)
```



## Plot `tpol`
```{r out.height = "450px"}
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
```





## Other parameters

Param       Purpose
--------    ----------------
`nobs`      The number of observations in the training set.
`mincut`    The minimum number of observations to include 
            in either child node. 
            This is a weighted quantity; 
            the observational weights are used 
            to compute the ‘number’. The default is 5.
`minsize`   The smallest allowed node size: 
            a weighted quantity. The default is 10.
`mindev`    The within-node deviance must be at least 
            this times that of the root node for the 
            node to be split.



## Grow a bigger tree

```{r}
tpol <- tree(
  formula = claim_count ~ discipline + state_group,
  data = pol_4tree,
  mindev = 0.001,
  minsize = 500)
```
```{r eval=FALSE}
print(tpol)
```
```{r echo=FALSE}
tree:::print.tree(tpol)
```





## Plot `tpol`
```{r out.height = "450px"}
plot(tpol, type = 'uniform')
text(tpol, pretty = 5, col = 'blue', cex = 0.8)
```




## Split by size
```{r}
pol_4tree$big <- (pol_4tree$revenue >= 4e6)
table(pol_4tree$big)
```



## Avg claims by revenue band

:::::::::::::: {.columns}
::: {.column}
```{r}
p1 <- pol_4tree %>% 
  mutate(rev_band = round(revenue/10e3, 0)) %>% 
  group_by(rev_band) %>% 
  summarize(avg_claim_count = mean(claim_count)) %>% 
  ggplot() + 
  aes(rev_band, avg_claim_count) +
  geom_point()
```
:::
::: {.column}
```{r echo=FALSE}
p1
```
:::
::::::::::::::





## What about `log(revenue)`?

:::::::::::::: {.columns}
::: {.column}
```{r}
pol_4tree$lrevenue <- log(pol_4tree$revenue)
summary(pol_4tree$lrevenue)
```
:::
::: {.column}
```{r}
ggplot(data = pol_4tree) +
  aes(lrevenue) +
  geom_histogram()
```
:::
::::::::::::::




## Avg claims by log(revenue) band

:::::::::::::: {.columns}
::: {.column}
```{r}
p2 <- pol_4tree %>% 
  mutate(lrev_band = round(lrevenue, 2)) %>% 
  group_by(lrev_band) %>% 
  summarize(avg_claim_count = mean(claim_count)) %>% 
  ggplot() + 
  aes(lrev_band, avg_claim_count) +
  geom_point()
```
:::
::: {.column}
```{r echo=FALSE}
p2
```
:::
::::::::::::::




## `tree` helped

**So, `tree` helped us to find that there is something
going on around $4m in revenue.**




## What are trees good for?

* Classification -- an alternative to logistic regression
* Regression -- but not good at prediction, better to use a forest
* Identifying interaction terms for `lm` and `glm`
* Find important variables -- forests are especially good at this
* Boosted trees perform even better than forests for prediction




## Cons

* Single trees are no good for stability or prediction
* Forests and boosted trees are harder to interpret




